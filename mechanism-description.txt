MECHANISM NOTES

----- Distributed Hash Table -----
For our structure, we were partly influenced by the Chord P2P lookup mechanism. We use the crc32 algorithm to do all of our hashing.  At boot, a node will create it's own groupList.
Given a shard_count, that node will create a groupList of ReplicaGroup objects. A ReplicaGroup object contains all of the state for
a shard, such as shard_id, key_count, a list of members inside that shard, and so on. Next, that node will add itself into one of the shards based the hash of it's 
own IP. It cycles through it's VIEW and does the same for every other replica. Placement of replicas uses a successor-like mechanism. If the hash(IP) < hash(group#), then the replica
with that respective IP will be placed into that ReplicaGroup (shard). In the events of a freshly booted node, we run the same type of initializing code but since we don't 
have a shard_count, we have to query another replica for it's current shard_count which we trust is the correct at the moment of the boot. Other logistical requests are sent, such as 
adding the new node to the right ReplicaGroup across all IP's that are alive, as well as adding it into their views, and etc.


----- DOWN REPLICAS -----
To treat down replicas, we simply look for timeouts. 
If a timeout occurs, we first have to call on repairView to adjust the view of the other replicas accordingly, which is removing the down replica from the view.
If that replica, or any other replica boots while others are running, we do the negative case of above, 
we fill each other replica's view with the new replicas socket address and then have that new replica send a get request
to our /key-value-store/ endpoint to retreive the entire KVS at a replica that is healthy.
With this, our new replica is functioning and has a live record of the store. The other replicas are also aware of this new replica as it is in their view at this point.
